25.6.13

## 1.解决了上海项目的两个新需求
## 2.完成了数学建模的一道题
## 3.读了一篇文章：Mix-of-Granularity: Optimize the Chunking Granularity for Retrieval-Augmented Generation

笔记如下：

提出​​混合粒度方法：通过路由模块基于输入查询动态确定知识源的最优信息粒度。该路由模块采用软标签损失函数进行高效训练。我们进一步将MoG扩展为​​图混合粒度，将参考文档预处理为图结构，实现对分散文本片段的检索。、

方法

MOG

将知识库文档按n种粒度分块（如 n=5），其中粒度1为最细（如单句），粒度 j(j>=2)由粒度 j-1的 2 个相邻块合并而成，形成层级化分块结构。

对每个粒度下的片段，使用BM25算法计算与查询 q 的相似度分数，衡量语义相关性。

每个粒度提取前k个高相关片段（如 (k=3)），形成总大小为 n x k的候选片段池。

通过 RoBERTa 将查询 q 编码为向量，经路由器映射为权重向量w，维度与n一致，表征各粒度的重要性。

将各粒度下片段的 BM25 分数与 w 加权整合，突出最优粒度的片段相关性。

软标签构造（用于训练mlp）

对于每个查询 q，使用 BM25从每个粒度级别的参考文档中检索最相关的片段（S_best），然后通过静态模型（包括 TF-IDF、RoBERTa或命中率得分计算 S_best 中每个片段与标签 l 的语义相似度，并存储于 sim_best。我们为 S_best 中相似度最高（次高）的片段分配 0.8（0.2）的软标签，其余片段补 0。路由器通过最小化二元交叉熵损失函数进行训练

MoG 的两步选择策略：

第一步：从 最细粒度 中选 top-k 相关块（chunk_r，最细粒度的单元）；
第二步：找到 最优粒度g_r 中 包含chunk_r的块 ，作为最终检索结果。


MOGG

MoG 的局限：仅通过调整粒度处理相邻片段，无法有效应对复杂问题中分散在不同段落 / 文档的信息（如跨源推理需求）。

将文档拆分为 1-2 个句子的节点，基于 BM25 计算节点相似度，超过阈值 Tgraph 则连边，构建语义关联图。

延续 MoG 的多粒度路由框架，仅将分块逻辑从 “线性窗口” 改为 “图跳跃范围”，其余组件（如路由器、软标签训练）不变。




